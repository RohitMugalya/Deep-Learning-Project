{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ucimlrepo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYR1eGtG5z-V",
        "outputId": "080d2c2c-b40d-4036-da16-54ba2adae191"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2025.10.5)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n",
            "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FWEjl3B5KWh",
        "outputId": "bac68b1e-f002-4843-e5e8-dc5a94ac3d19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Data loaded: Train=398, Val=85, Test=86\n",
            "\n",
            "Initializing population...\n",
            " Init 1/6: Prep=standard|all | Arch=U64TD0.5 | U16TD0.2 | U8TD0.2 | val_acc=0.9529\n",
            " Init 2/6: Prep=none|all+poly | Arch=U64SD0.3 | U32RD0.5 | U128RD0.3 | val_acc=0.6235\n",
            " Init 3/6: Prep=minmax|pca30 | Arch=U16RD0.1 | U64SD0.3 | val_acc=0.6235\n",
            " Init 4/6: Prep=standard|pca25 | Arch=U16SD0.3 | val_acc=0.6235\n",
            " Init 5/6: Prep=robust|pca10+poly | Arch=U8RD0.5 | val_acc=0.6941\n",
            " Init 6/6: Prep=minmax|all+poly | Arch=U16SD0.3 | U16SD0.0 | U32RD0.3 | val_acc=0.6235\n",
            "\n",
            "=== Generation 1 ===\n",
            " Best gen 1: Prep=standard|all | Arch=U64TD0.5 | U16TD0.2 | U8TD0.2 | val_acc=0.9529\n",
            "\n",
            "=== Generation 2 ===\n",
            " Best gen 2: Prep=standard|all | Arch=U64TD0.5 | U16TD0.2 | U8TD0.2 | val_acc=0.9529\n",
            "\n",
            "=== Generation 3 ===\n",
            " Best gen 3: Prep=standard|all | Arch=U64TD0.5 | U64TD0.5 | val_acc=0.9647\n",
            "\n",
            "=== Generation 4 ===\n",
            " Best gen 4: Prep=standard|all | Arch=U64TD0.5 | U64TD0.5 | val_acc=0.9647\n",
            "\n",
            "=== Final Best ===\n",
            " Best Preprocessing: standard|all\n",
            " Best Architecture: U64TD0.5 | U64TD0.5\n",
            " Val Acc: 0.9647 | Test Acc: 0.9767\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Evolutionary NAS with Preprocessing Search - Breast Cancer Dataset\n",
        "\"\"\"\n",
        "\n",
        "import copy\n",
        "import random\n",
        "from collections import namedtuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# ----------------------------\n",
        "# Config / Representation\n",
        "# ----------------------------\n",
        "NUM_CLASSES = 2\n",
        "Arch = list  # list of dicts for layers\n",
        "\n",
        "def random_preprocessing():\n",
        "    \"\"\"Generate random preprocessing configuration\"\"\"\n",
        "    return {\n",
        "        'scaler': random.choice(['standard', 'minmax', 'robust', 'none']),\n",
        "        'feature_selection': random.choice(['none', 'pca', 'selectk']),\n",
        "        'n_features': random.choice([10, 15, 20, 25, 30]),  # for PCA or SelectKBest\n",
        "        'add_polynomial': random.choice([False, True]),\n",
        "    }\n",
        "\n",
        "def random_layer(input_dim, min_units=8, max_units=128):\n",
        "    return {\n",
        "        'units': random.choice([8, 16, 32, 64, 128]),\n",
        "        'activation': random.choice(['relu', 'tanh', 'sigmoid']),\n",
        "        'dropout': random.choice([0.0, 0.1, 0.2, 0.3, 0.5])\n",
        "    }\n",
        "\n",
        "def random_arch(input_dim, min_layers=1, max_layers=4):\n",
        "    return [random_layer(input_dim) for _ in range(random.randint(min_layers, max_layers))]\n",
        "\n",
        "def arch_to_str(arch):\n",
        "    return ' | '.join(f\"U{l['units']}{l['activation'][0].upper()}D{l['dropout']}\" for l in arch)\n",
        "\n",
        "def preprocess_to_str(prep):\n",
        "    feat = f\"{prep['feature_selection']}{prep['n_features']}\" if prep['feature_selection'] != 'none' else 'all'\n",
        "    poly = '+poly' if prep['add_polynomial'] else ''\n",
        "    return f\"{prep['scaler']}|{feat}{poly}\"\n",
        "\n",
        "# ----------------------------\n",
        "# Preprocessing Pipeline\n",
        "# ----------------------------\n",
        "class PreprocessingPipeline:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.scaler = None\n",
        "        self.feature_transformer = None\n",
        "        self.n_output_features = None\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        X = X_train.copy()\n",
        "\n",
        "        # Scaling\n",
        "        if self.config['scaler'] == 'standard':\n",
        "            self.scaler = StandardScaler()\n",
        "        elif self.config['scaler'] == 'minmax':\n",
        "            self.scaler = MinMaxScaler()\n",
        "        elif self.config['scaler'] == 'robust':\n",
        "            self.scaler = RobustScaler()\n",
        "        else:\n",
        "            self.scaler = None\n",
        "\n",
        "        if self.scaler:\n",
        "            X = self.scaler.fit_transform(X)\n",
        "\n",
        "        # Polynomial features\n",
        "        if self.config['add_polynomial']:\n",
        "            X_poly = X ** 2\n",
        "            X = np.hstack([X, X_poly])\n",
        "\n",
        "        # Feature selection/reduction\n",
        "        if self.config['feature_selection'] == 'pca':\n",
        "            n_comp = min(self.config['n_features'], X.shape[1])\n",
        "            self.feature_transformer = PCA(n_components=n_comp)\n",
        "            X = self.feature_transformer.fit_transform(X)\n",
        "        elif self.config['feature_selection'] == 'selectk':\n",
        "            k = min(self.config['n_features'], X.shape[1])\n",
        "            self.feature_transformer = SelectKBest(f_classif, k=k)\n",
        "            X = self.feature_transformer.fit_transform(X, y_train.ravel())\n",
        "\n",
        "        self.n_output_features = X.shape[1]\n",
        "        return X\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_trans = X.copy()\n",
        "\n",
        "        if self.scaler:\n",
        "            X_trans = self.scaler.transform(X_trans)\n",
        "\n",
        "        if self.config['add_polynomial']:\n",
        "            X_poly = X_trans ** 2\n",
        "            X_trans = np.hstack([X_trans, X_poly])\n",
        "\n",
        "        if self.feature_transformer:\n",
        "            X_trans = self.feature_transformer.transform(X_trans)\n",
        "\n",
        "        return X_trans\n",
        "\n",
        "# ----------------------------\n",
        "# Model builder\n",
        "# ----------------------------\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, arch: Arch, input_dim, num_classes=NUM_CLASSES):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        layers = []\n",
        "        cur_dim = input_dim\n",
        "\n",
        "        for layer_config in arch:\n",
        "            layers.append(nn.Linear(cur_dim, layer_config['units']))\n",
        "\n",
        "            if layer_config['activation'] == 'relu':\n",
        "                layers.append(nn.ReLU())\n",
        "            elif layer_config['activation'] == 'tanh':\n",
        "                layers.append(nn.Tanh())\n",
        "            elif layer_config['activation'] == 'sigmoid':\n",
        "                layers.append(nn.Sigmoid())\n",
        "\n",
        "            if layer_config['dropout'] > 0:\n",
        "                layers.append(nn.Dropout(layer_config['dropout']))\n",
        "\n",
        "            cur_dim = layer_config['units']\n",
        "\n",
        "        layers.append(nn.Linear(cur_dim, num_classes))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# ----------------------------\n",
        "# Weight inheritance helper\n",
        "# ----------------------------\n",
        "def try_inherit_weights(child: nn.Module, parent: nn.Module):\n",
        "    child_dict = child.state_dict()\n",
        "    parent_dict = parent.state_dict()\n",
        "    matched = 0\n",
        "    for k, v in parent_dict.items():\n",
        "        if k in child_dict and child_dict[k].shape == v.shape:\n",
        "            child_dict[k] = v.clone()\n",
        "            matched += 1\n",
        "    child.load_state_dict(child_dict)\n",
        "    return matched\n",
        "\n",
        "# ----------------------------\n",
        "# Data loading\n",
        "# ----------------------------\n",
        "def load_breast_cancer_data():\n",
        "    breast_cancer = fetch_ucirepo(id=17)\n",
        "    X = breast_cancer.data.features\n",
        "    y = breast_cancer.data.targets\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X = X.values\n",
        "    y = y.values.ravel()\n",
        "\n",
        "    # Convert labels to 0 and 1 (M=1, B=0)\n",
        "    y = (y == 'M').astype(int)\n",
        "\n",
        "    # Split into train, val, test\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "# ----------------------------\n",
        "# Training utils\n",
        "# ----------------------------\n",
        "def train_one_epoch(model, device, X_train, y_train, optimizer, criterion, batch_size=32):\n",
        "    model.train()\n",
        "    n_samples = len(X_train)\n",
        "    indices = np.random.permutation(n_samples)\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i in range(0, n_samples, batch_size):\n",
        "        batch_idx = indices[i:i+batch_size]\n",
        "        X_batch = torch.FloatTensor(X_train[batch_idx]).to(device)\n",
        "        y_batch = torch.LongTensor(y_train[batch_idx]).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * len(batch_idx)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += len(batch_idx)\n",
        "        correct += predicted.eq(y_batch).sum().item()\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    acc = correct / total\n",
        "    return avg_loss, acc\n",
        "\n",
        "def evaluate(model, device, X, y, criterion=None, batch_size=32):\n",
        "    model.eval()\n",
        "    n_samples = len(X)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss_sum = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, n_samples, batch_size):\n",
        "            X_batch = torch.FloatTensor(X[i:i+batch_size]).to(device)\n",
        "            y_batch = torch.LongTensor(y[i:i+batch_size]).to(device)\n",
        "\n",
        "            outputs = model(X_batch)\n",
        "            if criterion:\n",
        "                loss = criterion(outputs, y_batch)\n",
        "                loss_sum += loss.item() * len(y_batch)\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += len(y_batch)\n",
        "            correct += predicted.eq(y_batch).sum().item()\n",
        "\n",
        "    acc = correct / total\n",
        "    avg_loss = loss_sum / total if criterion else None\n",
        "    return avg_loss, acc\n",
        "\n",
        "# ----------------------------\n",
        "# Evolutionary algorithm\n",
        "# ----------------------------\n",
        "Individual = namedtuple('Individual', ['preprocessing', 'arch', 'fitness', 'model_state', 'pipeline'])\n",
        "\n",
        "def mutate_arch(arch: Arch, max_layers=6):\n",
        "    new = copy.deepcopy(arch)\n",
        "    ops = ['add', 'remove', 'modify']\n",
        "    op = random.choice(ops)\n",
        "\n",
        "    if op == 'add' and len(new) < max_layers:\n",
        "        pos = random.randint(0, len(new))\n",
        "        new.insert(pos, random_layer(0))\n",
        "    elif op == 'remove' and len(new) > 1:\n",
        "        pos = random.randrange(len(new))\n",
        "        new.pop(pos)\n",
        "    else:\n",
        "        pos = random.randrange(len(new))\n",
        "        field = random.choice(['units', 'activation', 'dropout'])\n",
        "        if field == 'units':\n",
        "            new[pos]['units'] = random.choice([8, 16, 32, 64, 128])\n",
        "        elif field == 'activation':\n",
        "            new[pos]['activation'] = random.choice(['relu', 'tanh', 'sigmoid'])\n",
        "        else:\n",
        "            new[pos]['dropout'] = random.choice([0.0, 0.1, 0.2, 0.3, 0.5])\n",
        "\n",
        "    return new\n",
        "\n",
        "def mutate_preprocessing(prep):\n",
        "    new = copy.deepcopy(prep)\n",
        "    field = random.choice(['scaler', 'feature_selection', 'n_features', 'add_polynomial'])\n",
        "\n",
        "    if field == 'scaler':\n",
        "        new['scaler'] = random.choice(['standard', 'minmax', 'robust', 'none'])\n",
        "    elif field == 'feature_selection':\n",
        "        new['feature_selection'] = random.choice(['none', 'pca', 'selectk'])\n",
        "    elif field == 'n_features':\n",
        "        new['n_features'] = random.choice([10, 15, 20, 25, 30])\n",
        "    else:\n",
        "        new['add_polynomial'] = not new['add_polynomial']\n",
        "\n",
        "    return new\n",
        "\n",
        "def evolve(population, X_train, y_train, X_val, y_val, device, args):\n",
        "    population = sorted(population, key=lambda x: x.fitness if x.fitness is not None else 0.0, reverse=True)\n",
        "    next_pop = []\n",
        "    K = max(1, int(args.elitism * len(population)))\n",
        "    next_pop.extend(population[:K])\n",
        "\n",
        "    while len(next_pop) < args.pop_size:\n",
        "        tournament = random.sample(population, k=min(args.tournament_k, len(population)))\n",
        "        parent = max(tournament, key=lambda x: x.fitness if x.fitness is not None else 0.0)\n",
        "\n",
        "        # Mutate both preprocessing and architecture\n",
        "        if random.random() < 0.5:\n",
        "            child_prep = mutate_preprocessing(parent.preprocessing)\n",
        "            child_arch = parent.arch\n",
        "        else:\n",
        "            child_prep = parent.preprocessing\n",
        "            child_arch = mutate_arch(parent.arch, max_layers=args.max_layers)\n",
        "\n",
        "        # Apply preprocessing\n",
        "        pipeline = PreprocessingPipeline(child_prep)\n",
        "        X_train_proc = pipeline.fit(X_train, y_train)\n",
        "        X_val_proc = pipeline.transform(X_val)\n",
        "\n",
        "        # Build and train model\n",
        "        input_dim = X_train_proc.shape[1]\n",
        "        child_model = SimpleMLP(child_arch, input_dim).to(device)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(child_model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
        "\n",
        "        for ep in range(args.train_epochs):\n",
        "            train_one_epoch(child_model, device, X_train_proc, y_train, optimizer, criterion, args.batch_size)\n",
        "\n",
        "        _, val_acc = evaluate(child_model, device, X_val_proc, y_val, None, args.batch_size)\n",
        "        child_state = child_model.state_dict()\n",
        "        child = Individual(preprocessing=child_prep, arch=child_arch, fitness=val_acc,\n",
        "                         model_state=child_state, pipeline=pipeline)\n",
        "        next_pop.append(child)\n",
        "\n",
        "    return next_pop\n",
        "\n",
        "# ----------------------------\n",
        "# Main Evolution Run\n",
        "# ----------------------------\n",
        "def run_evolution(args):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    # Load data\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = load_breast_cancer_data()\n",
        "    print(f\"Data loaded: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
        "\n",
        "    # Initialize population\n",
        "    population = []\n",
        "    print(\"\\nInitializing population...\")\n",
        "    for i in range(args.pop_size):\n",
        "        prep = random_preprocessing()\n",
        "        arch = random_arch(30, min_layers=args.min_layers, max_layers=args.init_max_layers)\n",
        "\n",
        "        # Apply preprocessing\n",
        "        pipeline = PreprocessingPipeline(prep)\n",
        "        X_train_proc = pipeline.fit(X_train, y_train)\n",
        "        X_val_proc = pipeline.transform(X_val)\n",
        "\n",
        "        # Build and train model\n",
        "        input_dim = X_train_proc.shape[1]\n",
        "        model = SimpleMLP(arch, input_dim).to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
        "\n",
        "        for ep in range(args.init_train_epochs):\n",
        "            train_one_epoch(model, device, X_train_proc, y_train, optimizer, criterion, args.batch_size)\n",
        "\n",
        "        _, val_acc = evaluate(model, device, X_val_proc, y_val, None, args.batch_size)\n",
        "        state = model.state_dict()\n",
        "        population.append(Individual(preprocessing=prep, arch=arch, fitness=val_acc,\n",
        "                                    model_state=state, pipeline=pipeline))\n",
        "        print(f\" Init {i+1}/{args.pop_size}: Prep={preprocess_to_str(prep)} | Arch={arch_to_str(arch)} | val_acc={val_acc:.4f}\")\n",
        "\n",
        "    best = None\n",
        "    for gen in range(1, args.generations + 1):\n",
        "        print(f\"\\n=== Generation {gen} ===\")\n",
        "        population = evolve(population, X_train, y_train, X_val, y_val, device, args)\n",
        "        population = sorted(population, key=lambda x: x.fitness if x.fitness is not None else 0.0, reverse=True)\n",
        "        best = population[0]\n",
        "        print(f\" Best gen {gen}: Prep={preprocess_to_str(best.preprocessing)} | Arch={arch_to_str(best.arch)} | val_acc={best.fitness:.4f}\")\n",
        "\n",
        "    print(\"\\n=== Final Best ===\")\n",
        "    # Reconstruct best model with best preprocessing\n",
        "    X_train_proc = best.pipeline.fit(X_train, y_train)\n",
        "    X_test_proc = best.pipeline.transform(X_test)\n",
        "\n",
        "    input_dim = X_train_proc.shape[1]\n",
        "    best_model = SimpleMLP(best.arch, input_dim).to(device)\n",
        "    best_model.load_state_dict(best.model_state)\n",
        "\n",
        "    _, test_acc = evaluate(best_model, device, X_test_proc, y_test, None, args.batch_size)\n",
        "    print(f\" Best Preprocessing: {preprocess_to_str(best.preprocessing)}\")\n",
        "    print(f\" Best Architecture: {arch_to_str(best.arch)}\")\n",
        "    print(f\" Val Acc: {best.fitness:.4f} | Test Acc: {test_acc:.4f}\")\n",
        "    return best\n",
        "\n",
        "# ----------------------------\n",
        "# Fixed Arguments\n",
        "# ----------------------------\n",
        "class Args:\n",
        "    pop_size = 6\n",
        "    generations = 4\n",
        "    train_epochs = 3\n",
        "    init_train_epochs = 3\n",
        "    batch_size = 32\n",
        "    lr = 0.001\n",
        "    elitism = 0.3\n",
        "    tournament_k = 3\n",
        "    min_layers = 1\n",
        "    init_max_layers = 3\n",
        "    max_layers = 5\n",
        "\n",
        "args = Args()\n",
        "best_individual = run_evolution(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EvoNAS (MLP) -- (Preprocessing, Architecture)"
      ],
      "metadata": {
        "id": "5isSimGa83xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Evolutionary NAS with Preprocessing Search - Breast Cancer Dataset (MLP)\n",
        "\"\"\"\n",
        "\n",
        "import copy\n",
        "import random\n",
        "from collections import namedtuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# ----------------------------\n",
        "# Config / Representation\n",
        "# ----------------------------\n",
        "NUM_CLASSES = 2\n",
        "Arch = list  # list of dicts for layers\n",
        "\n",
        "def random_preprocessing():\n",
        "    \"\"\"Generate random preprocessing configuration\"\"\"\n",
        "    return {\n",
        "        'scaler': random.choice(['standard', 'minmax', 'robust', 'none']),\n",
        "        'feature_selection': random.choice(['none', 'pca', 'selectk']),\n",
        "        'n_features': random.choice([10, 15, 20, 25, 30]),  # for PCA or SelectKBest\n",
        "        'add_polynomial': random.choice([False, True]),\n",
        "    }\n",
        "\n",
        "def random_layer(input_dim, min_units=8, max_units=128):\n",
        "    return {\n",
        "        'units': random.choice([8, 16, 32, 64, 128]),\n",
        "        'activation': random.choice(['relu', 'tanh', 'sigmoid']),\n",
        "        'dropout': random.choice([0.0, 0.1, 0.2, 0.3, 0.5])\n",
        "    }\n",
        "\n",
        "def random_arch(input_dim, min_layers=1, max_layers=4):\n",
        "    return [random_layer(input_dim) for _ in range(random.randint(min_layers, max_layers))]\n",
        "\n",
        "def arch_to_str(arch):\n",
        "    return ' | '.join(f\"U{l['units']}{l['activation'][0].upper()}D{l['dropout']}\" for l in arch)\n",
        "\n",
        "def preprocess_to_str(prep):\n",
        "    feat = f\"{prep['feature_selection']}{prep['n_features']}\" if prep['feature_selection'] != 'none' else 'all'\n",
        "    poly = '+poly' if prep['add_polynomial'] else ''\n",
        "    return f\"{prep['scaler']}|{feat}{poly}\"\n",
        "\n",
        "# ----------------------------\n",
        "# Preprocessing Pipeline\n",
        "# ----------------------------\n",
        "class PreprocessingPipeline:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.scaler = None\n",
        "        self.feature_transformer = None\n",
        "        self.n_output_features = None\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        X = X_train.copy()\n",
        "\n",
        "        # Scaling\n",
        "        if self.config['scaler'] == 'standard':\n",
        "            self.scaler = StandardScaler()\n",
        "        elif self.config['scaler'] == 'minmax':\n",
        "            self.scaler = MinMaxScaler()\n",
        "        elif self.config['scaler'] == 'robust':\n",
        "            self.scaler = RobustScaler()\n",
        "        else:\n",
        "            self.scaler = None\n",
        "\n",
        "        if self.scaler:\n",
        "            X = self.scaler.fit_transform(X)\n",
        "\n",
        "        # Polynomial features\n",
        "        if self.config['add_polynomial']:\n",
        "            X_poly = X ** 2\n",
        "            X = np.hstack([X, X_poly])\n",
        "\n",
        "        # Feature selection/reduction\n",
        "        if self.config['feature_selection'] == 'pca':\n",
        "            n_comp = min(self.config['n_features'], X.shape[1])\n",
        "            self.feature_transformer = PCA(n_components=n_comp)\n",
        "            X = self.feature_transformer.fit_transform(X)\n",
        "        elif self.config['feature_selection'] == 'selectk':\n",
        "            k = min(self.config['n_features'], X.shape[1])\n",
        "            self.feature_transformer = SelectKBest(f_classif, k=k)\n",
        "            X = self.feature_transformer.fit_transform(X, y_train.ravel())\n",
        "\n",
        "        self.n_output_features = X.shape[1]\n",
        "        return X\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_trans = X.copy()\n",
        "\n",
        "        if self.scaler:\n",
        "            X_trans = self.scaler.transform(X_trans)\n",
        "\n",
        "        if self.config['add_polynomial']:\n",
        "            X_poly = X_trans ** 2\n",
        "            X_trans = np.hstack([X_trans, X_poly])\n",
        "\n",
        "        if self.feature_transformer:\n",
        "            X_trans = self.feature_transformer.transform(X_trans)\n",
        "\n",
        "        return X_trans\n",
        "\n",
        "# ----------------------------\n",
        "# Multi-Layer Perceptron (MLP) Model\n",
        "# ----------------------------\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Layer Perceptron with configurable architecture.\n",
        "    Each layer has: Linear → Activation → Dropout\n",
        "    \"\"\"\n",
        "    def __init__(self, arch: Arch, input_dim, num_classes=NUM_CLASSES):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        cur_dim = input_dim\n",
        "\n",
        "        # Build hidden layers\n",
        "        for layer_config in arch:\n",
        "            # Linear transformation\n",
        "            layers.append(nn.Linear(cur_dim, layer_config['units']))\n",
        "\n",
        "            # Activation function\n",
        "            if layer_config['activation'] == 'relu':\n",
        "                layers.append(nn.ReLU())\n",
        "            elif layer_config['activation'] == 'tanh':\n",
        "                layers.append(nn.Tanh())\n",
        "            elif layer_config['activation'] == 'sigmoid':\n",
        "                layers.append(nn.Sigmoid())\n",
        "\n",
        "            # Dropout for regularization\n",
        "            if layer_config['dropout'] > 0:\n",
        "                layers.append(nn.Dropout(layer_config['dropout']))\n",
        "\n",
        "            cur_dim = layer_config['units']\n",
        "\n",
        "        # Output layer\n",
        "        layers.append(nn.Linear(cur_dim, num_classes))\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# ----------------------------\n",
        "# Weight inheritance helper\n",
        "# ----------------------------\n",
        "def try_inherit_weights(child: nn.Module, parent: nn.Module):\n",
        "    child_dict = child.state_dict()\n",
        "    parent_dict = parent.state_dict()\n",
        "    matched = 0\n",
        "    for k, v in parent_dict.items():\n",
        "        if k in child_dict and child_dict[k].shape == v.shape:\n",
        "            child_dict[k] = v.clone()\n",
        "            matched += 1\n",
        "    child.load_state_dict(child_dict)\n",
        "    return matched\n",
        "\n",
        "# ----------------------------\n",
        "# Data loading\n",
        "# ----------------------------\n",
        "def load_breast_cancer_data():\n",
        "    breast_cancer = fetch_ucirepo(id=17)\n",
        "    X = breast_cancer.data.features\n",
        "    y = breast_cancer.data.targets\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X = X.values\n",
        "    y = y.values.ravel()\n",
        "\n",
        "    # Convert labels to 0 and 1 (M=1, B=0)\n",
        "    y = (y == 'M').astype(int)\n",
        "\n",
        "    # Split into train, val, test\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "# ----------------------------\n",
        "# Training utils\n",
        "# ----------------------------\n",
        "def train_one_epoch(model, device, X_train, y_train, optimizer, criterion, batch_size=32):\n",
        "    model.train()\n",
        "    n_samples = len(X_train)\n",
        "    indices = np.random.permutation(n_samples)\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i in range(0, n_samples, batch_size):\n",
        "        batch_idx = indices[i:i+batch_size]\n",
        "        X_batch = torch.FloatTensor(X_train[batch_idx]).to(device)\n",
        "        y_batch = torch.LongTensor(y_train[batch_idx]).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * len(batch_idx)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += len(batch_idx)\n",
        "        correct += predicted.eq(y_batch).sum().item()\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    acc = correct / total\n",
        "    return avg_loss, acc\n",
        "\n",
        "def evaluate(model, device, X, y, criterion=None, batch_size=32):\n",
        "    model.eval()\n",
        "    n_samples = len(X)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss_sum = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, n_samples, batch_size):\n",
        "            X_batch = torch.FloatTensor(X[i:i+batch_size]).to(device)\n",
        "            y_batch = torch.LongTensor(y[i:i+batch_size]).to(device)\n",
        "\n",
        "            outputs = model(X_batch)\n",
        "            if criterion:\n",
        "                loss = criterion(outputs, y_batch)\n",
        "                loss_sum += loss.item() * len(y_batch)\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += len(y_batch)\n",
        "            correct += predicted.eq(y_batch).sum().item()\n",
        "\n",
        "    acc = correct / total\n",
        "    avg_loss = loss_sum / total if criterion else None\n",
        "    return avg_loss, acc\n",
        "\n",
        "# ----------------------------\n",
        "# Evolutionary algorithm\n",
        "# ----------------------------\n",
        "Individual = namedtuple('Individual', ['preprocessing', 'arch', 'fitness', 'model_state', 'pipeline'])\n",
        "\n",
        "def mutate_arch(arch: Arch, max_layers=6):\n",
        "    new = copy.deepcopy(arch)\n",
        "    ops = ['add', 'remove', 'modify']\n",
        "    op = random.choice(ops)\n",
        "\n",
        "    if op == 'add' and len(new) < max_layers:\n",
        "        pos = random.randint(0, len(new))\n",
        "        new.insert(pos, random_layer(0))\n",
        "    elif op == 'remove' and len(new) > 1:\n",
        "        pos = random.randrange(len(new))\n",
        "        new.pop(pos)\n",
        "    else:\n",
        "        pos = random.randrange(len(new))\n",
        "        field = random.choice(['units', 'activation', 'dropout'])\n",
        "        if field == 'units':\n",
        "            new[pos]['units'] = random.choice([8, 16, 32, 64, 128])\n",
        "        elif field == 'activation':\n",
        "            new[pos]['activation'] = random.choice(['relu', 'tanh', 'sigmoid'])\n",
        "        else:\n",
        "            new[pos]['dropout'] = random.choice([0.0, 0.1, 0.2, 0.3, 0.5])\n",
        "\n",
        "    return new\n",
        "\n",
        "def mutate_preprocessing(prep):\n",
        "    new = copy.deepcopy(prep)\n",
        "    field = random.choice(['scaler', 'feature_selection', 'n_features', 'add_polynomial'])\n",
        "\n",
        "    if field == 'scaler':\n",
        "        new['scaler'] = random.choice(['standard', 'minmax', 'robust', 'none'])\n",
        "    elif field == 'feature_selection':\n",
        "        new['feature_selection'] = random.choice(['none', 'pca', 'selectk'])\n",
        "    elif field == 'n_features':\n",
        "        new['n_features'] = random.choice([10, 15, 20, 25, 30])\n",
        "    else:\n",
        "        new['add_polynomial'] = not new['add_polynomial']\n",
        "\n",
        "    return new\n",
        "\n",
        "def evolve(population, X_train, y_train, X_val, y_val, device, args):\n",
        "    population = sorted(population, key=lambda x: x.fitness if x.fitness is not None else 0.0, reverse=True)\n",
        "    next_pop = []\n",
        "    K = max(1, int(args.elitism * len(population)))\n",
        "    next_pop.extend(population[:K])\n",
        "\n",
        "    while len(next_pop) < args.pop_size:\n",
        "        tournament = random.sample(population, k=min(args.tournament_k, len(population)))\n",
        "        parent = max(tournament, key=lambda x: x.fitness if x.fitness is not None else 0.0)\n",
        "\n",
        "        # Mutate both preprocessing and architecture\n",
        "        if random.random() < 0.5:\n",
        "            child_prep = mutate_preprocessing(parent.preprocessing)\n",
        "            child_arch = parent.arch\n",
        "        else:\n",
        "            child_prep = parent.preprocessing\n",
        "            child_arch = mutate_arch(parent.arch, max_layers=args.max_layers)\n",
        "\n",
        "        # Apply preprocessing\n",
        "        pipeline = PreprocessingPipeline(child_prep)\n",
        "        X_train_proc = pipeline.fit(X_train, y_train)\n",
        "        X_val_proc = pipeline.transform(X_val)\n",
        "\n",
        "        # Build and train MLP model\n",
        "        input_dim = X_train_proc.shape[1]\n",
        "        child_model = MLP(child_arch, input_dim).to(device)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(child_model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
        "\n",
        "        for ep in range(args.train_epochs):\n",
        "            train_one_epoch(child_model, device, X_train_proc, y_train, optimizer, criterion, args.batch_size)\n",
        "\n",
        "        _, val_acc = evaluate(child_model, device, X_val_proc, y_val, None, args.batch_size)\n",
        "        child_state = child_model.state_dict()\n",
        "        child = Individual(preprocessing=child_prep, arch=child_arch, fitness=val_acc,\n",
        "                         model_state=child_state, pipeline=pipeline)\n",
        "        next_pop.append(child)\n",
        "\n",
        "    return next_pop\n",
        "\n",
        "# ----------------------------\n",
        "# Main Evolution Run\n",
        "# ----------------------------\n",
        "def run_evolution(args):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"Device:\", device)\n",
        "    print(\"Using Multi-Layer Perceptron (MLP) architecture\")\n",
        "\n",
        "    # Load data\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = load_breast_cancer_data()\n",
        "    print(f\"Data loaded: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
        "\n",
        "    # Initialize population\n",
        "    population = []\n",
        "    print(\"\\nInitializing population...\")\n",
        "    for i in range(args.pop_size):\n",
        "        prep = random_preprocessing()\n",
        "        arch = random_arch(30, min_layers=args.min_layers, max_layers=args.init_max_layers)\n",
        "\n",
        "        # Apply preprocessing\n",
        "        pipeline = PreprocessingPipeline(prep)\n",
        "        X_train_proc = pipeline.fit(X_train, y_train)\n",
        "        X_val_proc = pipeline.transform(X_val)\n",
        "\n",
        "        # Build and train MLP model\n",
        "        input_dim = X_train_proc.shape[1]\n",
        "        model = MLP(arch, input_dim).to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
        "\n",
        "        for ep in range(args.init_train_epochs):\n",
        "            train_one_epoch(model, device, X_train_proc, y_train, optimizer, criterion, args.batch_size)\n",
        "\n",
        "        _, val_acc = evaluate(model, device, X_val_proc, y_val, None, args.batch_size)\n",
        "        state = model.state_dict()\n",
        "        population.append(Individual(preprocessing=prep, arch=arch, fitness=val_acc,\n",
        "                                    model_state=state, pipeline=pipeline))\n",
        "        print(f\" Init {i+1}/{args.pop_size}: Prep={preprocess_to_str(prep)} | Arch={arch_to_str(arch)} | val_acc={val_acc:.4f}\")\n",
        "\n",
        "    best = None\n",
        "    for gen in range(1, args.generations + 1):\n",
        "        print(f\"\\n=== Generation {gen} ===\")\n",
        "        population = evolve(population, X_train, y_train, X_val, y_val, device, args)\n",
        "        population = sorted(population, key=lambda x: x.fitness if x.fitness is not None else 0.0, reverse=True)\n",
        "        best = population[0]\n",
        "        print(f\" Best gen {gen}: Prep={preprocess_to_str(best.preprocessing)} | Arch={arch_to_str(best.arch)} | val_acc={best.fitness:.4f}\")\n",
        "\n",
        "    print(\"\\n=== Final Best ===\")\n",
        "    # Reconstruct best model with best preprocessing\n",
        "    X_train_proc = best.pipeline.fit(X_train, y_train)\n",
        "    X_test_proc = best.pipeline.transform(X_test)\n",
        "\n",
        "    input_dim = X_train_proc.shape[1]\n",
        "    best_model = MLP(best.arch, input_dim).to(device)\n",
        "    best_model.load_state_dict(best.model_state)\n",
        "\n",
        "    _, test_acc = evaluate(best_model, device, X_test_proc, y_test, None, args.batch_size)\n",
        "    print(f\" Best Preprocessing: {preprocess_to_str(best.preprocessing)}\")\n",
        "    print(f\" Best Architecture: {arch_to_str(best.arch)}\")\n",
        "    print(f\" Network Type: Multi-Layer Perceptron (MLP)\")\n",
        "    print(f\" Val Acc: {best.fitness:.4f} | Test Acc: {test_acc:.4f}\")\n",
        "    return best\n",
        "\n",
        "# ----------------------------\n",
        "# Fixed Arguments\n",
        "# ----------------------------\n",
        "class Args:\n",
        "    pop_size = 6\n",
        "    generations = 4\n",
        "    train_epochs = 3\n",
        "    init_train_epochs = 3\n",
        "    batch_size = 32\n",
        "    lr = 0.001\n",
        "    elitism = 0.3\n",
        "    tournament_k = 3\n",
        "    min_layers = 1\n",
        "    init_max_layers = 3\n",
        "    max_layers = 5\n",
        "\n",
        "args = Args()\n",
        "best_individual = run_evolution(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6QQX_WS66x-",
        "outputId": "7f29f281-f962-4164-f035-788939c00473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Using Multi-Layer Perceptron (MLP) architecture\n",
            "Data loaded: Train=398, Val=85, Test=86\n",
            "\n",
            "Initializing population...\n",
            " Init 1/6: Prep=none|pca30+poly | Arch=U16SD0.3 | U32TD0.1 | val_acc=0.8471\n",
            " Init 2/6: Prep=minmax|selectk20+poly | Arch=U32RD0.0 | val_acc=0.8706\n",
            " Init 3/6: Prep=minmax|pca20 | Arch=U8RD0.5 | val_acc=0.3765\n",
            " Init 4/6: Prep=standard|selectk15 | Arch=U64RD0.5 | U128TD0.5 | val_acc=0.9294\n",
            " Init 5/6: Prep=standard|selectk20 | Arch=U64SD0.3 | U128TD0.0 | val_acc=0.9294\n",
            " Init 6/6: Prep=none|pca15+poly | Arch=U128TD0.1 | U8RD0.0 | val_acc=0.8471\n",
            "\n",
            "=== Generation 1 ===\n",
            " Best gen 1: Prep=standard|selectk15 | Arch=U64RD0.5 | val_acc=0.9529\n",
            "\n",
            "=== Generation 2 ===\n",
            " Best gen 2: Prep=standard|selectk15 | Arch=U64RD0.5 | val_acc=0.9529\n",
            "\n",
            "=== Generation 3 ===\n",
            " Best gen 3: Prep=standard|selectk15 | Arch=U64RD0.5 | val_acc=0.9529\n",
            "\n",
            "=== Generation 4 ===\n",
            " Best gen 4: Prep=standard|selectk15 | Arch=U64RD0.5 | val_acc=0.9529\n",
            "\n",
            "=== Final Best ===\n",
            " Best Preprocessing: standard|selectk15\n",
            " Best Architecture: U64RD0.5\n",
            " Network Type: Multi-Layer Perceptron (MLP)\n",
            " Val Acc: 0.9529 | Test Acc: 0.9884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bWdq3JvN9FsE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}